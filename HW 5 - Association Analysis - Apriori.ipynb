{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): Musab Abdullah and Jibran Khalil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "\n",
    "Association analysis uses machine learning algorithms to extract hidden relationships from large datasets. In this assignment we'll be using one of the most commonly used algorithms for association rule mining - the Apriori algorithm.\n",
    "\n",
    "For the first part of this assignment, the dataset (`large_retail.txt`) that we are going to use has been adapted from the [Retail Market Basket Dataset](http://fimi.ua.ac.be/data/retail.pdf). The dataset contains transaction records supplied by an anonymous Belgian retail supermarket store. Each line in the file represents a separate transaction with the item ids separated by space. The dataset has 3000 transaction records and 99 different item ids.\n",
    "\n",
    "We also provide a smaller dataset (`small_retail.txt`) with 9 transactions and 5 different item ids along with the solutions. *You should first test your implementation on this dataset, before running it on the larger dataset.*\n",
    "\n",
    "The assignment will be **autograded**. We will use the `diff` command in linux to compare the output files. So please **check your answers** based on the given sample output files.\n",
    "\n",
    "**Implementation Hint:**\n",
    "\n",
    "- Use the `frozenset` data structure in Python (similar to `set` in functionality) to represent the itemsets because `frozenset` is a hashable data structure. You can maintain a dictionary that maps from the itemset (a `frozenset`) to its support count.\n",
    "\n",
    "For the second part of this assignment, the data we'll use comes from a bakery called \"The Bread Basket\", located in the historic center of Edinburgh. The dataset contains more than 9000 transactions from the bakery. The file contains the following columns:\n",
    "\n",
    "- Date. Categorical variable that tells us the date of the transactions (YYYY-MM-DD format). The column includes dates from 2016-10-30 to 2017-04-09.\n",
    "\n",
    "- Time. Categorical variable that tells us the time of the transactions (HH:MM:SS format).\n",
    "\n",
    "- Transaction. Quantitative variable that allows us to differentiate the transactions. The rows that share the same value in this field belong to the same transaction.\n",
    "\n",
    "- Item. Categorical variable with the products purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Apriori Algorithm from scratch\n",
    "\n",
    "Apriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. In this part, you'll be implementing this algorithm for generating the itemsets that occur enough times to meet the `min_sup` threshold. Based on these frequent itemsets you'll find association rules that have confidence meeting or exceeding the `min_conf` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports (you can add additional headers if you wish)\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        data = [sorted([int(x) for x in line.rstrip().split()]) for line in content]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 5],\n",
       " [2, 4],\n",
       " [2, 3],\n",
       " [1, 2, 4],\n",
       " [1, 3],\n",
       " [2, 3],\n",
       " [1, 3],\n",
       " [1, 2, 3, 5],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = load_dataset('small_retail.txt')\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Implement the function `create_1_itemsets` that takes as input the entire dataset and returns a list of all the 1-itemsets. For example, for `small_retail.txt` it should return:\n",
    "~~~\n",
    "[frozenset({1}),\n",
    " frozenset({2}),\n",
    " frozenset({3}),\n",
    " frozenset({4}),\n",
    " frozenset({5})]\n",
    " ~~~\n",
    " Please **don't hardcode** the item ids, your code should support item ids that are non-sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}),\n",
       " frozenset({2}),\n",
       " frozenset({3}),\n",
       " frozenset({4}),\n",
       " frozenset({5})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_1_itemsets(dataset):\n",
    "    items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            items.add(item)\n",
    "    result = [frozenset([x]) for x in items]\n",
    "    return result\n",
    "\n",
    "create_1_itemsets(small_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Implement function `filter_candidates` that takes as input the candidate itemsets, the dataset, and the minumum support count `min_sup`, and filters out candidates that don't meet the support threshold.\n",
    "\n",
    "**Hint:** You should also return the support count information (perhaps as a `dict`) for the itemsets. This will be useful later on for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{1, 3}, {1, 5}],\n",
       " {frozenset({1, 3}): 4, frozenset({1, 4}): 1, frozenset({1, 5}): 2})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_candidates(candidates, dataset, min_sup):\n",
    "    result = []\n",
    "    support_data = {}\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        sc = 0\n",
    "        candy = set(candidate)\n",
    "        for tr in dataset:\n",
    "            if candy.issubset(set(tr)):\n",
    "                sc += 1\n",
    "        support_data[frozenset(candy)] = sc\n",
    "        if sc >= min_sup:\n",
    "            result.append(candy)\n",
    "    \n",
    "    return result, support_data\n",
    "\n",
    "candidates = [[1, 3], [1, 4], [1, 5]]\n",
    "filter_candidates(candidates, small_dataset, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Implement the function `generate_next_itemsets` that takes in frequent itemsets of size `k` and generates candidate itemsets of size `k + 1`.\n",
    "\n",
    "**Hint:** Use either the F(k-1) x F(k-1) or the F(k-1) x F(1) candidate generation method. **Filter the candidate list based on the apriori principle before returning it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 3], [2, 3]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_next_itemsets(freq_sets):\n",
    "    candidates = []\n",
    "    if len(freq_sets) == 0: # precondition\n",
    "        return candidates\n",
    "    k = len(freq_sets[0])\n",
    "    \n",
    "    fo_sets = [sorted(itemset) for itemset in freq_sets] # frequent sets ordered\n",
    "    for i in range(len(fo_sets)):\n",
    "        for j in range(i + 1, len(fo_sets)):\n",
    "            a = len(fo_sets[i])\n",
    "            b = len(fo_sets[j])\n",
    "            if a == b and fo_sets[i][:a-1] == fo_sets[j][:b-1]: # if all but the last element of the sets are the same\n",
    "                candy = set(fo_sets[i]).union(fo_sets[j]) # merge the itemssets\n",
    "                candidates.append(candy)\n",
    "#     print('pre apriori pruning', candidates)\n",
    "\n",
    "    # apriori pruning\n",
    "    result = []\n",
    "    for candy in candidates:\n",
    "        subsets = list(itertools.combinations(candy, k)) # get all subsets of size k\n",
    "        fo_sets = [set(x) for x in fo_sets]\n",
    "        if all(set(subset) in fo_sets for subset in subsets): # check if all subsets are frequent\n",
    "            result.append(sorted(candy))\n",
    "    return result\n",
    "\n",
    "# generate_next_itemsets([[7, 9], [7, 10], [9, 10], [1, 3], [1, 5]])\n",
    "generate_next_itemsets([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Implement the function `apriori_freq_temsets` that takes the entire dataset as input and returns all the frequent itemsets that meet `min_sup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}), frozenset({2}), frozenset({3}), [1, 2], [1, 3], [2, 3]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apriori_freq_itemsets(dataset, minsup):\n",
    "    result = []\n",
    "    one_sets = create_1_itemsets(dataset)\n",
    "#     print(one_sets)\n",
    "    fone_sets, info = filter_candidates(one_sets, dataset, minsup)\n",
    "#     print(fone_sets, '\\n', info)\n",
    "    result.extend(fone_sets)\n",
    "    \n",
    "    k = 2\n",
    "    prev_sets = fone_sets\n",
    "    while True:\n",
    "        next_sets = generate_next_itemsets(prev_sets)\n",
    "        if len(next_sets) == 0:\n",
    "            break\n",
    "#         print(k, 'cands', next_sets)\n",
    "        fnext_sets, info = filter_candidates(next_sets, dataset, minsup)\n",
    "#         print('minsup filtered', [sorted(x) for x in fnext_sets])\n",
    "        result.extend([sorted(x) for x in fnext_sets])\n",
    "        prev_sets = next_sets\n",
    "        k += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "apriori_freq_itemsets(small_dataset, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Display the frequent item sets in the form of a table along with their `support` (as a fraction: support count over number of transactions) for the `large_retail.txt` dataset with min support count 300.\n",
    "\n",
    "Sample Table Format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.44\t[1, 2]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "`support(itemset) = support_count(itemset) / num_total_transactions`.\n",
    "\n",
    "The `support` and the itemset should be separated by a tab (`'\\t'`).\n",
    "\n",
    "Note that the `support` should be rounded to the nearest 2 decimal places (use `round(sup, 2)`). If a support_fraction only contains 1 decimal place (for example, 0.1), you do not need to add a 0 to the end of it (leaving it as 0.1 is fine).\n",
    "\n",
    "The itemsets should also be in a sorted order where smaller itemsets should come before larger itemsets and itemsets of the same size should be sorted amongst themselves.\n",
    "\n",
    "For eg. \n",
    "~~~~\n",
    "[1, 2] should come before [1, 2, 3]\n",
    "[1, 2, 3] should come before [1, 2, 4]\n",
    "[1, 2, 3] should come before [1, 4, 5]\n",
    "[1, 2, 3] should come before [2, 3, 4]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The output also **shouldn't contain any duplicates**. The sample output for the `small_retail.txt` dataset with min support count as 2 is:\n",
    "\n",
    "~~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.78\t[2]\n",
    "0.67\t[3]\n",
    "0.22\t[4]\n",
    "0.22\t[5]\n",
    "0.44\t[1, 2]\n",
    "0.44\t[1, 3]\n",
    "0.22\t[1, 5]\n",
    "0.44\t[2, 3]\n",
    "0.22\t[2, 4]\n",
    "0.22\t[2, 5]\n",
    "0.22\t[1, 2, 3]\n",
    "0.22\t[1, 2, 5]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `large_apriori_itemsets.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_itemsets.txt` for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "minsup = 300\n",
    "num_total_transactions = len(large_dataset)\n",
    "freq_sets = apriori_freq_itemsets(large_dataset, minsup)\n",
    "_, sup_counts = filter_candidates(freq_sets, large_dataset, minsup)\n",
    "\n",
    "f = open('large_apriori_itemsets.txt', 'w')\n",
    "f.write('Sup\\tFreq Itemset\\n')\n",
    "for itemset in sup_counts:\n",
    "    support = sup_counts[itemset] / num_total_transactions\n",
    "    f.write(f'{round(support, 2)}\\t{sorted(itemset)}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Find the closed frequent item sets along with their `support`. Store the results for the `large_retail.txt` dataset in the file `large_apriori_closed_itemsets.txt`, in the same format as specified in Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(frozenset({31}), 309)\n",
      "(frozenset({32}), 420)\n",
      "(frozenset({36}), 320)\n",
      "(frozenset({38}), 771)\n",
      "(frozenset({39}), 1592)\n",
      "(frozenset({41}), 669)\n",
      "(frozenset({48}), 1396)\n",
      "(frozenset({60}), 337)\n",
      "(frozenset({65}), 329)\n",
      "(frozenset({89}), 343)\n",
      "(frozenset({32, 39}), 420)\n",
      "(frozenset({38, 39}), 449)\n",
      "(frozenset({48, 38}), 384)\n",
      "(frozenset({41, 39}), 428)\n",
      "(frozenset({48, 39}), 982)\n",
      "(frozenset({48, 41}), 534)\n",
      "(frozenset({48, 41, 39}), 428)\n",
      "Sup\tFreq Itemset\n",
      "\n",
      "0.1\t[31]\n",
      "\n",
      "0.11\t[36]\n",
      "\n",
      "0.26\t[38]\n",
      "\n",
      "0.53\t[39]\n",
      "\n",
      "0.22\t[41]\n",
      "\n",
      "0.47\t[48]\n",
      "\n",
      "0.11\t[60]\n",
      "\n",
      "0.11\t[65]\n",
      "\n",
      "0.11\t[89]\n",
      "\n",
      "0.14\t[32, 39]\n",
      "\n",
      "0.15\t[38, 39]\n",
      "\n",
      "0.13\t[38, 48]\n",
      "\n",
      "0.33\t[39, 48]\n",
      "\n",
      "0.18\t[41, 48]\n",
      "\n",
      "0.14\t[39, 41, 48]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "dataset = large_dataset\n",
    "minsup = 300\n",
    "num_total_transactions = len(dataset)\n",
    "freq_sets = apriori_freq_itemsets(dataset, minsup)\n",
    "_, sup_counts = filter_candidates(freq_sets, dataset, minsup)\n",
    "\n",
    "for x in sup_counts.items():\n",
    "    print(x)\n",
    "\n",
    "def apriori_closed_sets(support_counts):\n",
    "    closed_sets = []\n",
    "    for fs in support_counts:\n",
    "        if not any(fs.issubset(c) for c in closed_sets):\n",
    "            for c in closed_sets:\n",
    "                if fs.issuperset(c):\n",
    "                    if support_counts[fs] == support_counts[c]:\n",
    "                        closed_sets.remove(c)\n",
    "            closed_sets.append(fs)\n",
    "\n",
    "        \n",
    "    return closed_sets\n",
    "                    \n",
    "closed = apriori_closed_sets(sup_counts)\n",
    "f = open('large_apriori_closed_itemsets.txt', 'w')\n",
    "f.write('Sup\\tFreq Itemset\\n')\n",
    "print('Sup\\tFreq Itemset\\n')\n",
    "for itemset in closed:\n",
    "    support = sup_counts[itemset] / num_total_transactions\n",
    "    print(f'{round(support, 2)}\\t{sorted(itemset)}\\n')\n",
    "    f.write(f'{round(support, 2)}\\t{sorted(itemset)}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (+5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Generate the rules having confidence above `min_conf = 0.5` using the frequent itemsets generated in Q5. Display the rules in the form of a table.\n",
    "\n",
    "Sample table format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Conf    Rule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "Note that rule confidence should be rounded to the nearest 2 decimal places (use `round(conf, 2)`). This table should also be tab (`'\\t'`) separated. The rules should be displayed in the sorted order. If a rule is given as `LHS -> RHS` then the rules for which `len(LHS)` is lesser should appear first. If the `len(LHS)` is equal for two rules then rules for which `len(RHS)` is lesser should appear first. If both `len(LHS)` and `len(RHS)` is equal then the rules should be sorted based on LHS first and then based on RHS.\n",
    "\n",
    "~~~~\n",
    "Note:\n",
    "LHS (Left Hand Side)\n",
    "RHS (Right Hand Side)\n",
    "~~~~\n",
    "\n",
    "For eg.\n",
    "~~~~\n",
    "[3] -> [2] should come before [1, 3] -> [4]\n",
    "[4] -> [2] should come before [2] -> [3, 4]\n",
    "[1, 3] -> [2] should come before [1, 5] -> [2]\n",
    "[1, 2] -> [3] should come before [1, 2] -> [5]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The sample output for the `small_retail.txt` dataset with `min_conf = 0.5` is:\n",
    "\n",
    "~~~~\n",
    "Sup\t Conf\tRule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.44\t0.67\t[1] -> [3]\n",
    "0.44\t0.57\t[2] -> [1]\n",
    "0.44\t0.57\t[2] -> [3]\n",
    "0.44\t0.67\t[3] -> [1]\n",
    "0.44\t0.67\t[3] -> [2]\n",
    "0.22\t1.0\t [4] -> [2]\n",
    "0.22\t1.0\t [5] -> [1]\n",
    "0.22\t1.0\t [5] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t0.5\t [1, 2] -> [3]\n",
    "0.22\t0.5\t [1, 2] -> [5]\n",
    "0.22\t0.5\t [1, 3] -> [2]\n",
    "0.22\t1.0\t [1, 5] -> [2]\n",
    "0.22\t0.5\t [2, 3] -> [1]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `large_apriori_rules.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_rules.txt` for your convenience.\n",
    "\n",
    "**Hint:** You shouldn't traverse the entire dataset to compute the confidence for a rule since you have already computed the `support_data` for all the frequent itemsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Apriori Algorithm using MLXtend\n",
    "\n",
    "## Installation\n",
    "`scikit-learn` does not provide any functionality for association rule mining so for this assignment you will be using the `MLxtend` library. The documentation for this library is available [here](http://rasbt.github.io/mlxtend/).\n",
    "\n",
    "You will need to install the `MLxtend` library. There are several ways of doing this; you can follow the instructions below, or see the setup guide  [here](http://rasbt.github.io/mlxtend/installation/).\n",
    "\n",
    "\n",
    "### Conda\n",
    "\n",
    "Most of you should use Conda to do the install. If you downladed Anaconda in order to get Jupyter Notebooks (which is the most common way to get Jupyter Notebooks), then you will want to install `MLxtend` using Conda. \n",
    "\n",
    "Open a command prompt / terminal window and type:\n",
    "\n",
    "`conda install mlxtend --channel conda-forge`\n",
    "\n",
    "### PyPi\n",
    "\n",
    "You can also install via pip. Note: If you are running Jupyter notebooks through an Anaconda install, then pip may not place the `MLxtend` libraries in the correct place for use in Jupyter. \n",
    "\n",
    "Open a command prompt / terminal window and type:\n",
    "\n",
    "`pip3 install mlxtend`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you'll be running this algorithm for generating the itemsets that occur more than the `min_sup` threshold. Based on these frequent itemsets you'll find association rules that have confidence above the `min_conf` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports (you can add additional headers if you wish)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()[1:]\n",
    "    transactions = []\n",
    "    prev_tid = -1\n",
    "    for t in content:\n",
    "        t = t.strip().split(',')[-2:]\n",
    "        tid = t[0]\n",
    "        item = t[1]\n",
    "        if prev_tid != tid:\n",
    "            prev_tid = tid\n",
    "            transactions.append([item])\n",
    "        else:\n",
    "            transactions[-1].append(item)\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num transactions: 9531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Bread'],\n",
       " ['Scandinavian', 'Scandinavian'],\n",
       " ['Hot chocolate', 'Jam', 'Cookies'],\n",
       " ['Muffin'],\n",
       " ['Coffee', 'Pastry', 'Bread'],\n",
       " ['Medialuna', 'Pastry', 'Muffin'],\n",
       " ['Medialuna', 'Pastry', 'Coffee', 'Tea'],\n",
       " ['Pastry', 'Bread'],\n",
       " ['Bread', 'Muffin'],\n",
       " ['Scandinavian', 'Medialuna']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('BreadBasket_DMS.csv')\n",
    "\n",
    "# ** NOTE: dataset is a 2D list (not a dataframe!) **\n",
    "\n",
    "print(\"Num transactions:\", len(dataset))\n",
    "#Print the first 10 transactions\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Many transactions in the dataset include the item \"NONE.\" First, find and remove all the \"NONE\" items from the dataset. There are some transactions that only contain \"NONE,\" so removing \"NONE\" will leave some transactions as empty lists. Remove all the empty lists as well. \n",
    "\n",
    "Once you have removed the NONEs, find the top 10 best-selling items in the bakery. Create a bar chart to display the total number of transactions for each of the top 10 selling items. Sort the bar chart by frequency (the top most sold item first, down to the 10th most sold item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** \n",
    "\n",
    "a.) Using `mlxtend.preprocessing.TransactionEncoder` transform `dataset` into an array format suitable for the `mlxtend` library. You will need to call `fit` then `transform`. \n",
    "\n",
    "`TransactionEncoder` learns unique items from the dataset and transforms each transaction into a one-hot encoded boolean numpy array. For example, the resulting encoded dataset will be represented by something like this, where each row is a transaction. If the first transaction contained ['Crepe', 'Jam'], this would correspond to the first row in the encoded table. \n",
    "\n",
    "<img src=\"table.png\">\n",
    "\n",
    "Print the `shape` of the resulting encoded numpy array.\n",
    "\n",
    "b.) `TransactionEncoder` also has a function `inverse_transform` that allows you to tranform one-hot encoded transactions back to the item labels. Try it out on the first 5 transactions and display the items in the first 5 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** Convert the encoded numpy array from Q9 part a into a pandas dataframe. Use the `TransactionEncoder`'s `.columns_` attribute as the column headers. Print the head of the resulting dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Use the `mlxtend.frequent_patterns.apriori` to generate the frequent itemsets with minimum support of 1%. Display these itemsets along with their support values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Use `mlxtend.frequent_patterns.fpmax` to find and display all of the maximal frequent itemsets along with their support values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Using the frequent itemsets and support counts in Q11, find all the closed frequent itemsets along with their support fraction. Also print a count of how many closed frequent itemsets you have found. (MLxtend does not provide a function to do this - you will need to write it yourself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** Use `mlxtend.frequent_patterns.association_rules` to calculate rules with a confidence level of 0.25 for the frequent itemsets you generated in Q11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** An important step in generating a set of association rules is to determine the optimal thresholds for support and confidence. If we set these values too low we will get a lot of rules and most of them will not be useful. Generate the frequent itemsets with minimum support of 0.5% and plot the number of rules generated with respect to the confidence threshold by varying its value between 0 and 1 with increments of 0.1. What happens when we increase the confidence level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16.** What value would you choose for the minimum confidence threshold based on the plot in Q15? Why? Display the rules generated for the your chosen value. Take a look at the generated rules. Are they interesting? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
